{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac48f4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 10:15:04,889\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "import pandas as pd\n",
    "import ray\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(object_store_memory=4000000000) # set object store memory to 4GB\n",
    "\n",
    "data = pd.read_pickle('C:\\\\Users\\\\manue\\\\switchdrive\\\\Mutual Funds Project\\\\data\\\\pickle_files\\\\full_dataset.pkl')\n",
    "sample = data.sample(101, random_state=1)\n",
    "\n",
    "# tansform 'Rating' from Categorical to float\n",
    "import numpy as np\n",
    "sample[\"Rating\"] = pd.to_numeric(sample[\"Rating\"], errors='coerce')\n",
    "\n",
    "# drop all rows with inf/-inf values!\n",
    "import numpy as np\n",
    "sample = sample[(sample != np.inf).all(axis=1)]\n",
    "sample = sample[(sample != -np.inf).all(axis=1)]\n",
    "\n",
    "#get rid of whitespace to draw tree later\n",
    "Eq_Stylebox = sample['Eq_Stylebox_Long'].astype('object').replace(' ','_', regex=True)\n",
    "sample['Eq_Stylebox_Long'] = Eq_Stylebox.astype('category')\n",
    "\n",
    "# shifting target variable to predict next month\n",
    "sample['returns'] = sample.returns.shift(-1)\n",
    "sample = sample.drop(sample.tail(1).index)\n",
    "\n",
    "\n",
    "X = sample.drop('returns', axis=1)\n",
    "y = sample['returns']\n",
    "\n",
    "# create dummies in case of categorical data\n",
    "dummy_needed = [#'Rating',\n",
    "                'Financial_Health_Grade_Long',\n",
    "                 'Growth_Grade_Long',\n",
    "                 'Profitability_Grade_Long',\n",
    "                 'Eq_Stylebox_Long']\n",
    "\n",
    "X = pd.get_dummies(X, columns=dummy_needed)\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 26)\n",
    "#X_train = ray.put(X_train)\n",
    "#y_train = ray.put(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.0001), np.log(0.9)),\n",
    "    \"max_depth\": hp.choice(\"max_depth\", range(1, 16)),\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", range(100, 10000)),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "    \"colsample_bylevel\": hp.uniform(\"colsample_bylevel\", 0.5, 1),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.001), np.log(100)),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.001), np.log(100)),\n",
    "}\n",
    "\n",
    "\n",
    "kfolds = KFold(n_splits=5, shuffle=True, random_state=26)\n",
    "\n",
    "# Define the objective function to optimize\n",
    "def objective(config):\n",
    "    model = xgb.XGBRegressor(**config, n_jobs=-1)\n",
    "    score = cross_val_score(model, X=ray.get(X_train), y=ray.get(y_train), scoring=\"neg_mean_squared_error\", cv=kfolds)\n",
    "    rmse = np.sqrt(-np.mean(score))\n",
    "    tune.report(rmse=rmse)\n",
    "\n",
    "# Define the search algorithm\n",
    "search_alg = HyperOptSearch(space=search_space, metric=\"rmse\", mode=\"min\")\n",
    "# to limit number of cores, uncomment and set max_concurrent \n",
    "# algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "\n",
    "# Define the hyperparameter tuning trials object\n",
    "trials = Trials()\n",
    "\n",
    "# Define the configuration for Ray Tune\n",
    "config = {\n",
    "    'num_samples': 10,\n",
    "    'config': search_space,\n",
    "    'search_alg': search_alg,\n",
    "    'scheduler': scheduler,\n",
    "    'resources_per_trial': {'cpu': 1},\n",
    "    'metric': 'rmse',\n",
    "    'mode': 'min',\n",
    "    'verbose': 1,\n",
    "    'name': 'xgboost_tuning',\n",
    "    'stop': {'training_iteration': 10},\n",
    "    'local_dir': './ray_results',\n",
    "}\n",
    "\n",
    "# Start the hyperparameter tuning using Ray Tune\n",
    "analysis = tune.run(objective, **config)\n",
    "\n",
    "# Print the best hyperparameters found during the search\n",
    "best_params = analysis.get_best_config(metric='rmse')\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7fcce4",
   "metadata": {},
   "source": [
    "### Distributed Ray XGB\n",
    "https://github.com/ray-project/xgboost_ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4e994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from xgboost_ray import RayDMatrix, RayParams, train\n",
    "from ray import tune\n",
    "\n",
    "def train_xgboost(config, checkpoint_dir=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_set = RayDMatrix(X_train, y_train)\n",
    "    test_set = RayDMatrix(X_test, y_test)\n",
    "    \n",
    "    evals_result = {}\n",
    "    \n",
    "    # train the model\n",
    "    bst = train(\n",
    "        params=config,\n",
    "        dtrain=train_set,\n",
    "        evals_result=evals_result,\n",
    "        evals=[(test_set, \"eval\")],\n",
    "        verbose_eval=False,\n",
    "        num_boost_round=5, #equivalent to 'epochs'\n",
    "        ray_params=RayParams(num_actors=4, cpus_per_actor=2)) #parameters for parallelism\n",
    "    \n",
    "    model_path = 'model.xgb'\n",
    "    bst.save_model(model_path)\n",
    "    print(f'total time taken: {time.time()-start_time}')\n",
    "    print('Final rmse: {:.4f}'.format(\n",
    "    evals_result[\"eval\"][\"rmse\"][-1]))\n",
    "    \n",
    "    return bst\n",
    "    \n",
    "\n",
    "# Define the search algorithm and scheduler\n",
    "#### NOT SURE IF NEEDED OR SUPPORTED IN XGB_RAY! maybe uncomment here and in config\n",
    "search_alg = HyperOptSearch()\n",
    "scheduler = ASHAScheduler(max_t=10, grace_period=1)\n",
    "\n",
    "# Specify the hyperparameter search space.\n",
    "config = {\n",
    "    \"tree_method\": \"approx\",\n",
    "    \"objective\": \"reg:squarederror\", # Use regression objective\n",
    "    \"eval_metric\": \"rmse\", # Set the evaluation metric to RMSE\n",
    "    \"learning_rate\": tune.loguniform(1e-4, 0.9),\n",
    "    \"subsample\": tune.uniform(0.8, 1.0),\n",
    "    \"colsample_bytree\": tune.uniform(0.8, 1.0),\n",
    "    \"colsample_bylevel\": tune.uniform(0.8, 1.0),\n",
    "    \"max_depth\": tune.randint(1, 16),\n",
    "    \"n_estimators\": tune.randint(500, 10000),\n",
    "    \"reg_alpha\": tune.loguniform(1e-3, 100),\n",
    "    \"reg_lambda\": tune.loguniform(1e-3, 100),\n",
    "}\n",
    "\n",
    "# Run the hyperparameter search\n",
    "analysis = tune.run(\n",
    "    train_xgboost,\n",
    "    resources_per_trial=RayParams(num_actors=4, cpus_per_actor=2).get_tune_resources(),\n",
    "    config=config,\n",
    "    num_samples=5,\n",
    "    search_alg=search_alg,\n",
    "    scheduler=scheduler,\n",
    "    metric='rmse',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_RMSE = analysis.best_result[\"rmse\"]\n",
    "print(f'Best model parameters: {analysis.best_config}')\n",
    "print(f'Best RMSE: {best_RMSE}')\n",
    "print(analysis.best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d42edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
