{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500ecbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 10:02:04,926\tERROR services.py:1169 -- Failed to start the dashboard , return code 1\n",
      "2023-04-19 10:02:04,926\tERROR services.py:1194 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.\n",
      "2023-04-19 10:02:04,942\tERROR services.py:1238 -- \n",
      "The last 20 lines of C:\\Users\\manue\\AppData\\Local\\Temp\\ray\\session_2023-04-19_10-02-02_390797_10616\\logs\\dashboard.log (it contains the error message from the dashboard): \n",
      "  File \"C:\\Users\\manue\\anaconda3\\lib\\site-packages\\ray\\dashboard\\utils.py\", line 121, in get_all_modules\n",
      "    importlib.import_module(name)\n",
      "  File \"C:\\Users\\manue\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\manue\\anaconda3\\lib\\site-packages\\ray\\dashboard\\modules\\job\\cli.py\", line 14, in <module>\n",
      "    from ray.job_submission import JobStatus, JobSubmissionClient\n",
      "  File \"C:\\Users\\manue\\anaconda3\\lib\\site-packages\\ray\\job_submission\\__init__.py\", line 2, in <module>\n",
      "    from ray.dashboard.modules.job.pydantic_models import DriverInfo, JobDetails, JobType\n",
      "  File \"C:\\Users\\manue\\anaconda3\\lib\\site-packages\\ray\\dashboard\\modules\\job\\pydantic_models.py\", line 4, in <module>\n",
      "    from pydantic import BaseModel, Field\n",
      "  File \"pydantic\\__init__.py\", line 2, in init pydantic.__init__\n",
      "  File \"pydantic\\dataclasses.py\", line 39, in init pydantic.dataclasses\n",
      "ImportError: cannot import name dataclass_transform\n",
      "2023-04-19 10:02:05,103\tINFO worker.py:1553 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Exception ignored in: <function Server.__del__ at 0x000002234F936F70>\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   File \"C:\\Users\\manue\\anaconda3\\lib\\site-packages\\grpc\\aio\\_server.py\", line 170, in __del__\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m TypeError: 'NoneType' object is not callable\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-19 10:02:11,416 E 19424 12016] (raylet.exe) agent_manager.cc:135: The raylet exited immediately because the Ray agent failed. The raylet fate shares with the agent. This can happen because the Ray agent was unexpectedly killed or failed. Agent can fail when\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m - The version of `grpcio` doesn't follow Ray's requirement. Agent can segfault with the incorrect `grpcio` version. Check the grpcio version `pip freeze | grep grpcio`.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m - The agent failed to start because of unexpected error or port conflict. Read the log `cat /tmp/ray/session_latest/dashboard_agent.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m - The agent is killed by the OS (e.g., out of memory).\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m *** SIGTERM received at time=1681891331 ***\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     @   00007FF6A4BDD0C7  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     @   00007FF6A4BDB82E  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     @   00007FF919E59363  (unknown)  recalloc\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     @   00007FF91B10269D  (unknown)  BaseThreadInitThunk\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     @   00007FF91C34A9F8  (unknown)  RtlUserThreadStart\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-19 10:02:11,628 E 19424 12016] (raylet.exe) logging.cc:361: *** SIGTERM received at time=1681891331 ***\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-19 10:02:11,628 E 19424 12016] (raylet.exe) logging.cc:361:     @   00007FF6A4BDD0C7  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-19 10:02:11,628 E 19424 12016] (raylet.exe) logging.cc:361:     @   00007FF6A4BDB82E  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-19 10:02:11,628 E 19424 12016] (raylet.exe) logging.cc:361:     @   00007FF919E59363  (unknown)  recalloc\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-19 10:02:11,628 E 19424 12016] (raylet.exe) logging.cc:361:     @   00007FF91B10269D  (unknown)  BaseThreadInitThunk\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-19 10:02:11,628 E 19424 12016] (raylet.exe) logging.cc:361:     @   00007FF91C34A9F8  (unknown)  RtlUserThreadStart\n",
      "2023-04-19 10:02:25,681\tWARNING worker.py:1866 -- The node with node id: c0023623c44cf4911d634ef74d9eeb8b490131060e4c06f15018d1bc and address: 127.0.0.1 and node name: 127.0.0.1 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a \t(1) raylet crashes unexpectedly (OOM, preempted node, etc.) \n",
      "\t(2) raylet has lagging heartbeats due to slow network or busy workload.\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "import pandas as pd\n",
    "import ray\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(object_store_memory=4000000000) # set object store memory to 4GB\n",
    "\n",
    "data = pd.read_pickle('C:\\\\Users\\\\manue\\\\switchdrive\\\\Mutual Funds Project\\\\data\\\\pickle_files\\\\full_dataset.pkl')\n",
    "sample = data.sample(50001, random_state=123)\n",
    "\n",
    "# tansform 'Rating' from Categorical to float\n",
    "import numpy as np\n",
    "sample[\"Rating\"] = pd.to_numeric(sample[\"Rating\"], errors='coerce')\n",
    "\n",
    "# drop all rows with inf/-inf values!\n",
    "import numpy as np\n",
    "sample = sample[(sample != np.inf).all(axis=1)]\n",
    "sample = sample[(sample != -np.inf).all(axis=1)]\n",
    "\n",
    "#get rid of whitespace to draw tree later\n",
    "Eq_Stylebox = sample['Eq_Stylebox_Long'].astype('object').replace(' ','_', regex=True)\n",
    "sample['Eq_Stylebox_Long'] = Eq_Stylebox.astype('category')\n",
    "\n",
    "# shifting target variable to predict next month\n",
    "sample['returns'] = sample.returns.shift(-1)\n",
    "sample = sample.drop(sample.tail(1).index)\n",
    "\n",
    "\n",
    "X = sample.drop('returns', axis=1)\n",
    "y = sample['returns']\n",
    "\n",
    "# create dummies in case of categorical data\n",
    "dummy_needed = [#'Rating',\n",
    "                'Financial_Health_Grade_Long',\n",
    "                 'Growth_Grade_Long',\n",
    "                 'Profitability_Grade_Long',\n",
    "                 'Eq_Stylebox_Long']\n",
    "\n",
    "X = pd.get_dummies(X, columns=dummy_needed)\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 26)\n",
    "#X_train = ray.put(X_train)\n",
    "#y_train = ray.put(y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9a55519",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from keras import EarlyStopping\n",
    "from keras.optimizers import Adam, SGD\n",
    "from sklearn.model_selection import KFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Define the function to optimize\n",
    "def optimize_nn(hidden_size_1, hidden_size_2, dropout_rate_1, dropout_rate_2, learning_rate, optimizer):\n",
    "    \n",
    "    # Define the K-fold cross-validation parameters\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize the array to store the validation accuracy for each fold\n",
    "    val_accs = np.zeros(n_splits)\n",
    "    \n",
    "    # Loop over the folds\n",
    "    fold = 0\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        # Split the data into training and validation sets for this fold\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_val, y_val = X[val_idx], y[val_idx]\n",
    "        \n",
    "        # Define the model architecture with the given hyperparameters\n",
    "        inputs = Input(shape=(X_train.shape[1],))\n",
    "        x = Dense(int(hidden_size_1), activation='relu')(inputs)\n",
    "        x = Dropout(dropout_rate_1)(x)\n",
    "        x = Dense(int(hidden_size_2), activation='relu')(x)\n",
    "        x = Dropout(dropout_rate_2)(x)\n",
    "        outputs = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        # Compile the model with the given learning rate and optimizer\n",
    "        if optimizer == 'adam':\n",
    "            optimizer = Adam(lr=learning_rate)\n",
    "        elif optimizer == 'sgd':\n",
    "            optimizer = SGD(lr=learning_rate, momentum=0.9)\n",
    "        else:\n",
    "            raise ValueError('Invalid optimizer')\n",
    "        model.compile(loss='mse', optimizer=optimizer)\n",
    "        \n",
    "        # Fit the model on the training data with early stopping on the validation loss\n",
    "        #early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)\n",
    "        \n",
    "        # Evaluate the model on the validation data for this fold\n",
    "        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        val_accs[fold] = val_acc\n",
    "        \n",
    "        # Increment the fold counter\n",
    "        fold += 1\n",
    "    \n",
    "    # Return the mean validation accuracy over all folds\n",
    "    return np.mean(val_accs)\n",
    "\n",
    "# Define the hyperparameter bounds to search over\n",
    "pbounds = {\n",
    "    'hidden_size_1': (16, 256),\n",
    "    'hidden_size_2': (16, 256),\n",
    "    'dropout_rate_1': (0.0, 0.5),\n",
    "    'dropout_rate_2': (0.0, 0.5),\n",
    "    'learning_rate': (1e-5, 1e-2),\n",
    "    'optimizer': ['adam', 'sgd']\n",
    "}\n",
    "\n",
    "# Define the BayesianOptimization object with the objective function to optimize\n",
    "optimizer = BayesianOptimization(f=optimize_nn, pbounds=pbounds, random_state=42)\n",
    "\n",
    "# Run the optimization for 20 iterations with verbose output\n",
    "optimizer.maximize(n_iter=20, verbose=2)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print('Best hyperparameters:', optimizer.max['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a30b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "import numpy as np\n",
    "\n",
    "# Define the training function for Ray Tune\n",
    "def train_model(config):\n",
    "    # Define the neural network model\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(X_train.shape[1], config['dense_layer_size']),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(config['dropout_rate']),\n",
    "        nn.Linear(config['dense_layer_size'], config['dense_layer_size']),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(config['dropout_rate']),\n",
    "        nn.Linear(config['dense_layer_size'], 1)\n",
    "    )\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(config['epochs']):\n",
    "        # Forward pass\n",
    "        y_pred = model(X_train)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = torch.sqrt(nn.MSELoss()(y_pred, y_train))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the testing set\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        test_loss = torch.sqrt(nn.MSELoss()(y_pred, y_test))\n",
    "\n",
    "    # Return the RMSE test loss\n",
    "    return {'rmse': test_loss.item()}\n",
    "\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    'dense_layer_size': tune.choice([32, 64, 128]),\n",
    "    'dropout_rate': tune.uniform(0, 0.5),\n",
    "    'learning_rate': tune.loguniform(1e-5, 1e-2),\n",
    "    'epochs': tune.choice([50, 100, 200])\n",
    "}\n",
    "\n",
    "# Configure Ray Tune\n",
    "analysis = tune.run(\n",
    "    train_model,\n",
    "    config=search_space,\n",
    "    num_samples=100,\n",
    "    scheduler=HyperBandScheduler(),\n",
    "    metric='rmse',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(f'Best hyperparameters: {analysis.best_config}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
